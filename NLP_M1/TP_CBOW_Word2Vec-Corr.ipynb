{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP — Implémentation de Word2Vec (CBOW) en PyTorch\n",
    "\n",
    "**Objectif :** Dans ce TP, nous allons construire **à la main** un modèle Word2Vec en version **CBOW (Continuous Bag of Words)**.\n",
    "\n",
    "L’objectif est d’apprendre des **représentations vectorielles** (embeddings) de mots en entraînant un petit réseau de neurones :\n",
    "\n",
    "$$\\mathbb{P}(w_t \\mid h_t) = \\text{Softmax}(W^{(2)} \\cdot h_t)$$\n",
    "\n",
    "où :\n",
    "\n",
    "- la moyenne des embeddings des mots du **contexte** est donnée par $$ h_t = \\frac{1}{2C} \\sum_{i=-C, i\\neq 0}^{C} v(w_{t+i}), $$\n",
    "- $ v(w) \\in \\mathbb{R}^p $ est le vecteur du mot,\n",
    "- $ W^{(1)} \\in \\mathbb{R}^{p \\times N} $ et $ W^{(2)} \\in \\mathbb{R}^{N \\times p} $,\n",
    "- et la **loss** est l’anti-log-vraisemblance :\n",
    "\n",
    "$$\\mathcal{L} = - \\sum_{t=1}^{T} \\log \\mathbb{P}(w_t \\mid h_t)$$\n",
    "\n",
    "Nous travaillerons sur un corpus réduit issu de *WikiText-2*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import des bibliothèques et prétraitement du corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = pd.read_csv(\n",
    "    './wikitext-2-train.csv',\n",
    "    header=None,\n",
    "    names=['text'],\n",
    "    encoding='utf-8'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tokenizer model \n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence):\n",
    "    return [token.text.lower() for token in nlp(sentence) if token.is_alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 615/615 [03:19<00:00,  3.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple : ['york', 'city', 'season', 'the', 'season', 'was', 'the', 'unk', 'season', 'of']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = [tokenize_sentence(t) for t in tqdm(train_iter['text'].dropna().tolist())]\n",
    "sentences = [s for s in sentences if len(s) > 2]\n",
    "print(\"Exemple :\", sentences[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construction du vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du vocabulaire : 19402\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocab = Counter([w for sent in sentences for w in sent])\n",
    "vocab = {w: c for w, c in vocab.items() if c >= 5}\n",
    "word2idx = {w: i for i, w in enumerate(vocab.keys())}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "V = len(word2idx)\n",
    "print(f\"Taille du vocabulaire : {V}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Génération des couples (contexte, mot cible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple : ([0, 1, 2, 3, 2, 3, 5, 2, 6, 7], 4)\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 5\n",
    "\n",
    "def generate_context_target(sentences, context_size=5):\n",
    "    \"\"\"\n",
    "    Generate (context, target) training pairs for the CBOW model.\n",
    "\n",
    "    For each word in each sentence, this function extracts:\n",
    "        - A target word (the central word)\n",
    "        - Its surrounding context words within a specified window size\n",
    "\n",
    "    Example:\n",
    "        Sentence: [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\"]\n",
    "        context_size = 2\n",
    "        → Target = \"brown\"\n",
    "        → Context = [\"the\", \"quick\", \"fox\", \"jumps\"]\n",
    "\n",
    "    Args:\n",
    "        sentences (list of list of str): List of tokenized sentences\n",
    "        context_size (int): Number of context words to take on each side of the target\n",
    "\n",
    "    Returns:\n",
    "        list of tuples: Each tuple (context, target) contains\n",
    "                        - context: list of word indices (size = 2 * context_size)\n",
    "                        - target: integer index of the center word\n",
    "\n",
    "    Meaning:\n",
    "        The result is a training dataset for the CBOW neural network, where each\n",
    "        example teaches the model to predict the central word given its surrounding\n",
    "        context words.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for sentence in sentences:\n",
    "        indices = [word2idx[w] for w in sentence if w in word2idx]\n",
    "        for i in range(context_size, len(indices) - context_size):\n",
    "            context = indices[i - context_size:i] + indices[i + 1:i + context_size + 1]\n",
    "            target = indices[i]\n",
    "            data.append((context, target))\n",
    "    return data\n",
    "\n",
    "data = generate_context_target(sentences, CONTEXT_SIZE)\n",
    "print(\"Exemple :\", data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Définition du Dataset PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for the Continuous Bag-of-Words (CBOW) model.\n",
    "    Each sample consists of a context (list of word indices) and a target word index.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with preprocessed (context, target) pairs.\n",
    "\n",
    "        Args:\n",
    "            data (list of tuples): Each tuple contains (context_indices, target_index)\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the total number of (context, target) samples.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of samples in the dataset\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve the context-target pair at the given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the desired sample\n",
    "\n",
    "        Returns:\n",
    "            tuple: (context_tensor, target_tensor)\n",
    "        \"\"\"\n",
    "        context, target = self.data[idx]\n",
    "        return torch.tensor(context, dtype=torch.long), torch.tensor(target, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CBOWDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Définition du modèle CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    \"\"\"\n",
    "    Continuous Bag-of-Words (CBOW) neural network model.\n",
    "\n",
    "    This model predicts a target word given the embeddings of its surrounding context words.\n",
    "    It consists of two linear transformations:\n",
    "        - W1: word embedding lookup table\n",
    "        - W2: projection from embedding space back to vocabulary space\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        \"\"\"\n",
    "        Initialize CBOW model parameters.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Number of unique words in the vocabulary\n",
    "            embedding_dim (int): Dimensionality of the embedding space\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.W2 = nn.Linear(embedding_dim, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, context_words):\n",
    "        \"\"\"\n",
    "        Forward pass of the CBOW model.\n",
    "\n",
    "        Args:\n",
    "            context_words (Tensor): Tensor of shape (batch_size, 2C)\n",
    "                containing indices of context words\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Log-probabilities over the vocabulary for each target word\n",
    "        \"\"\"\n",
    "        embeds = self.W1(context_words)  # (batch_size, 2C, embedding_dim)\n",
    "        h = embeds.mean(dim=1)           # Average context embeddings\n",
    "        out = self.W2(h)                 # Project to vocabulary space\n",
    "        log_probs = torch.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW(\n",
      "  (W1): Embedding(19402, 100)\n",
      "  (W2): Linear(in_features=100, out_features=19402, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "model = CBOW(V, embedding_dim)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Boucle d’apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 317687.5695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Loss: 355964.0588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Loss: 364986.5849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m model(context)\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(log_probs, target)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     14\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/sigbert-env/lib/python3.8/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/sigbert-env/lib/python3.8/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=False)\n",
    "    for context, target in loop:\n",
    "        optimizer.zero_grad()\n",
    "        log_probs = model(context)\n",
    "        loss = loss_fn(log_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exploration des embeddings appris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(word, top_k=5):\n",
    "    \"\"\"\n",
    "    Display the top-k most similar and least similar words to a given word\n",
    "    based on cosine similarity in the embedding space.\n",
    "\n",
    "    Args:\n",
    "        word (str): The query word.\n",
    "        top_k (int): Number of top and bottom words to display.\n",
    "    \"\"\"\n",
    "    if word not in word2idx:\n",
    "        print(\"Mot hors vocabulaire.\")\n",
    "        return\n",
    "\n",
    "    idx = word2idx[word]\n",
    "    w_vec = model.W1.weight[idx]\n",
    "\n",
    "    # Compute cosine similarities between the target word and all embeddings\n",
    "    sims = F.cosine_similarity(model.W1.weight, w_vec.unsqueeze(0))\n",
    "\n",
    "    # Get top positive and negative similarities\n",
    "    best = torch.topk(sims, top_k + 1)           # +1 to skip the word itself\n",
    "    worst = torch.topk(-sims, top_k)\n",
    "\n",
    "    print(f\"\\nMost similar and dissimilar words to '{word}':\\n\")\n",
    "    print(f\"{'Most similar':<30}{'Most dissimilar':<30}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Pair them together and print\n",
    "    for (i_pos, score_pos), (i_neg, score_neg) in zip(\n",
    "        zip(best.indices[1:], best.values[1:]),\n",
    "        zip(worst.indices, -worst.values)\n",
    "    ):\n",
    "        print(f\"{idx2word[i_pos.item()]:<15} cosine={score_pos.item():>6.3f}   \"\n",
    "              f\"{idx2word[i_neg.item()]:<15} cosine={score_neg.item():>6.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                                   Word: king                                   \n",
      "================================================================================\n",
      "\n",
      "Most similar and dissimilar words to 'king':\n",
      "\n",
      "Most similar                  Most dissimilar               \n",
      "------------------------------------------------------------\n",
      "monarch         cosine= 0.497   dolan           cosine=-0.343\n",
      "annals          cosine= 0.448   schedules       cosine=-0.337\n",
      "governor        cosine= 0.445   particles       cosine=-0.307\n",
      "lordship        cosine= 0.425   televised       cosine=-0.301\n",
      "hairan          cosine= 0.410   au              cosine=-0.295\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "                                  Word: piano                                   \n",
      "================================================================================\n",
      "\n",
      "Most similar and dissimilar words to 'piano':\n",
      "\n",
      "Most similar                  Most dissimilar               \n",
      "------------------------------------------------------------\n",
      "ode             cosine= 0.374   portuguese      cosine=-0.326\n",
      "stabilized      cosine= 0.372   katra           cosine=-0.320\n",
      "keyboard        cosine= 0.356   ignorant        cosine=-0.312\n",
      "burnley         cosine= 0.355   ledger          cosine=-0.308\n",
      "avalon          cosine= 0.352   yat             cosine=-0.295\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "                                  Word: doctor                                  \n",
      "================================================================================\n",
      "\n",
      "Most similar and dissimilar words to 'doctor':\n",
      "\n",
      "Most similar                  Most dissimilar               \n",
      "------------------------------------------------------------\n",
      "diverged        cosine= 0.388   taiwan          cosine=-0.350\n",
      "spectators      cosine= 0.383   old             cosine=-0.341\n",
      "gender          cosine= 0.383   grade           cosine=-0.320\n",
      "cure            cosine= 0.374   atia            cosine=-0.317\n",
      "priesthood      cosine= 0.373   flooded         cosine=-0.313\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "                                 Word: musician                                 \n",
      "================================================================================\n",
      "\n",
      "Most similar and dissimilar words to 'musician':\n",
      "\n",
      "Most similar                  Most dissimilar               \n",
      "------------------------------------------------------------\n",
      "singer          cosine= 0.479   gustav          cosine=-0.320\n",
      "syndicate       cosine= 0.418   isabela         cosine=-0.311\n",
      "blowin          cosine= 0.411   immunity        cosine=-0.302\n",
      "cheap           cosine= 0.411   marlon          cosine=-0.300\n",
      "duet            cosine= 0.406   undeniable      cosine=-0.299\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "                                  Word: money                                   \n",
      "================================================================================\n",
      "\n",
      "Most similar and dissimilar words to 'money':\n",
      "\n",
      "Most similar                  Most dissimilar               \n",
      "------------------------------------------------------------\n",
      "salvation       cosine= 0.423   inc             cosine=-0.375\n",
      "work            cosine= 0.395   gertrude        cosine=-0.338\n",
      "merchants       cosine= 0.393   norfolk         cosine=-0.321\n",
      "wiggs           cosine= 0.380   summed          cosine=-0.315\n",
      "compensation    cosine= 0.379   cdt             cosine=-0.312\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "example_words = [\"king\", \"piano\", \"doctor\", \"musician\", \"money\"]\n",
    "\n",
    "for example in example_words:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"{'Word: ' + example:^80}\")\n",
    "    print(\"=\" * 80)\n",
    "    find_nearest(example, top_k=5)\n",
    "    print(\"\\n\" + \"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "Nous avons implémenté :\n",
    "- la **tokenisation** et la **création du vocabulaire**,\n",
    "- la **génération des couples (contexte, cible)**,\n",
    "- un **réseau de neurones CBOW** entraîné par *negative log likelihood*,\n",
    "- et l’exploration qualitative des embeddings.\n",
    "\n",
    "Ce TP illustre les fondements de l’apprentissage distributionnel du langage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
