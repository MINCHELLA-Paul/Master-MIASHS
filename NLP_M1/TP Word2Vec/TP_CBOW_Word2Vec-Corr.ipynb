{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP — Implémentation de Word2Vec (CBOW) en PyTorch\n",
    "\n",
    "**Objectif :** Dans ce TP, nous allons construire **à la main** un modèle Word2Vec en version **CBOW (Continuous Bag of Words)**.\n",
    "\n",
    "L’objectif est d’apprendre des **représentations vectorielles** (embeddings) de mots en entraînant un petit réseau de neurones :\n",
    "\n",
    "$$\\mathbb{P}(w_t \\mid h_t) = \\text{Softmax}(W^{(2)} \\cdot h_t)$$\n",
    "\n",
    "où :\n",
    "\n",
    "- la moyenne des embeddings des mots du **contexte** est donnée par $$ h_t = \\frac{1}{2C} \\sum_{i=-C, i\\neq 0}^{C} v(w_{t+i}), $$\n",
    "- $ v(w) \\in \\mathbb{R}^p $ est le vecteur du mot,\n",
    "- $ W^{(1)} \\in \\mathbb{R}^{p \\times N} $ et $ W^{(2)} \\in \\mathbb{R}^{N \\times p} $,\n",
    "- et la **loss** est l’anti-log-vraisemblance :\n",
    "\n",
    "$$\\mathcal{L} = - \\sum_{t=1}^{T} \\log \\mathbb{P}(w_t \\mid h_t)$$\n",
    "\n",
    "Nous travaillerons sur un corpus réduit issu de *WikiText-2*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import des bibliothèques et prétraitement du corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = pd.read_csv(\n",
    "    './wikitext-2-train.csv',\n",
    "    header=None,\n",
    "    names=['text'],\n",
    "    encoding='utf-8'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tokenizer model \n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence):\n",
    "    return [token.text.lower() for token in nlp(sentence) if token.is_alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 615/615 [03:20<00:00,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple : ['york', 'city', 'season', 'the', 'season', 'was', 'the', 'unk', 'season', 'of']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = [tokenize_sentence(t) for t in tqdm(train_iter['text'].dropna().tolist())]\n",
    "sentences = [s for s in sentences if len(s) > 2]\n",
    "print(\"Exemple :\", sentences[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construction du vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du vocabulaire : 19402\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocab = Counter([w for sent in sentences for w in sent])\n",
    "vocab = {w: c for w, c in vocab.items() if c >= 5}\n",
    "word2idx = {w: i for i, w in enumerate(vocab.keys())}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "V = len(word2idx)\n",
    "print(f\"Taille du vocabulaire : {V}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Génération des couples (contexte, mot cible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple : ([0, 1, 2, 2, 4, 3], 3)\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 3\n",
    "\n",
    "def generate_context_target(sentences, context_size=5):\n",
    "    \"\"\"\n",
    "    Generate (context, target) training pairs for the CBOW model.\n",
    "\n",
    "    For each word in each sentence, this function extracts:\n",
    "        - A target word (the central word)\n",
    "        - Its surrounding context words within a specified window size\n",
    "\n",
    "    Example:\n",
    "        Sentence: [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\"]\n",
    "        context_size = 2\n",
    "        → Target = \"brown\"\n",
    "        → Context = [\"the\", \"quick\", \"fox\", \"jumps\"]\n",
    "\n",
    "    Args:\n",
    "        sentences (list of list of str): List of tokenized sentences\n",
    "        context_size (int): Number of context words to take on each side of the target\n",
    "\n",
    "    Returns:\n",
    "        list of tuples: Each tuple (context, target) contains\n",
    "                        - context: list of word indices (size = 2 * context_size)\n",
    "                        - target: integer index of the center word\n",
    "\n",
    "    Meaning:\n",
    "        The result is a training dataset for the CBOW neural network, where each\n",
    "        example teaches the model to predict the central word given its surrounding\n",
    "        context words.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for sentence in sentences:\n",
    "        indices = [word2idx[w] for w in sentence if w in word2idx]\n",
    "        for i in range(context_size, len(indices) - context_size):\n",
    "            context = indices[i - context_size:i] + indices[i + 1:i + context_size + 1]\n",
    "            target = indices[i]\n",
    "            data.append((context, target))\n",
    "    return data\n",
    "\n",
    "data = generate_context_target(sentences, CONTEXT_SIZE)\n",
    "print(\"Exemple :\", data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Définition du Dataset PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for the Continuous Bag-of-Words (CBOW) model.\n",
    "    Each sample consists of a context (list of word indices) and a target word index.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with preprocessed (context, target) pairs.\n",
    "\n",
    "        Args:\n",
    "            data (list of tuples): Each tuple contains (context_indices, target_index)\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the total number of (context, target) samples.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of samples in the dataset\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve the context-target pair at the given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the desired sample\n",
    "\n",
    "        Returns:\n",
    "            tuple: (context_tensor, target_tensor)\n",
    "        \"\"\"\n",
    "        context, target = self.data[idx]\n",
    "        return torch.tensor(context, dtype=torch.long), torch.tensor(target, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CBOWDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Définition du modèle CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    \"\"\"\n",
    "    Continuous Bag-of-Words (CBOW) neural network model.\n",
    "\n",
    "    This model predicts a target word given the embeddings of its surrounding context words.\n",
    "    It consists of two linear transformations:\n",
    "        - W1: word embedding lookup table\n",
    "        - W2: projection from embedding space back to vocabulary space\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        \"\"\"\n",
    "        Initialize CBOW model parameters.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Number of unique words in the vocabulary\n",
    "            embedding_dim (int): Dimensionality of the embedding space\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.W2 = nn.Linear(embedding_dim, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, context_words):\n",
    "        \"\"\"\n",
    "        Forward pass of the CBOW model.\n",
    "\n",
    "        Args:\n",
    "            context_words (Tensor): Tensor of shape (batch_size, 2C)\n",
    "                containing indices of context words\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Log-probabilities over the vocabulary for each target word\n",
    "        \"\"\"\n",
    "        embeds = self.W1(context_words)  # (batch_size, 2C, embedding_dim)\n",
    "        h = embeds.mean(dim=1)           # Average context embeddings\n",
    "        out = self.W2(h)                 # Project to vocabulary space\n",
    "        log_probs = torch.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW(\n",
      "  (W1): Embedding(19402, 100)\n",
      "  (W2): Linear(in_features=100, out_features=19402, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "model = CBOW(V, embedding_dim)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Boucle d’apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/sigbert-env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Loss: 20810.0363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Loss: 17957.8131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Loss: 16805.4065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "EPOCHS = 3\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=False)\n",
    "    for context, target in loop:\n",
    "        optimizer.zero_grad()\n",
    "        log_probs = model(context)\n",
    "        loss = loss_fn(log_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exploration des embeddings appris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(word, top_k=5):\n",
    "    \"\"\"\n",
    "    Display the top-k most similar and least similar words to a given word\n",
    "    based on cosine similarity in the embedding space.\n",
    "\n",
    "    Args:\n",
    "        word (str): The query word.\n",
    "        top_k (int): Number of top and bottom words to display.\n",
    "    \"\"\"\n",
    "    if word not in word2idx:\n",
    "        print(\"Mot hors vocabulaire.\")\n",
    "        return\n",
    "\n",
    "    idx = word2idx[word]\n",
    "    w_vec = model.W1.weight[idx]\n",
    "\n",
    "    # Compute cosine similarities between the target word and all embeddings\n",
    "    sims = F.cosine_similarity(model.W1.weight, w_vec.unsqueeze(0))\n",
    "\n",
    "    # Get top positive and negative similarities\n",
    "    best = torch.topk(sims, top_k + 1)           # +1 to skip the word itself\n",
    "    worst = torch.topk(-sims, top_k)\n",
    "\n",
    "    print(f\"\\nMost similar and dissimilar words to '{word}':\\n\")\n",
    "    print(f\"{'Most similar':<30}{'Most dissimilar':<30}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Pair them together and print\n",
    "    for (i_pos, score_pos), (i_neg, score_neg) in zip(\n",
    "        zip(best.indices[1:], best.values[1:]),\n",
    "        zip(worst.indices, -worst.values)\n",
    "    ):\n",
    "        print(f\"{idx2word[i_pos.item()]:<15} cosine={score_pos.item():>6.3f}   \"\n",
    "              f\"{idx2word[i_neg.item()]:<15} cosine={score_neg.item():>6.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                                   Word: king                                   \n",
      "================================================================================\n",
      "\n",
      "Most similar and dissimilar words to 'king':\n",
      "\n",
      "Most similar                  Most dissimilar               \n",
      "------------------------------------------------------------\n",
      "kingship        cosine= 0.461   steal           cosine=-0.293\n",
      "anne            cosine= 0.435   sanctioned      cosine=-0.282\n",
      "martyr          cosine= 0.396   creative        cosine=-0.281\n",
      "tim             cosine= 0.388   quiet           cosine=-0.272\n",
      "césar           cosine= 0.385   adaptable       cosine=-0.262\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "                                  Word: piano                                   \n",
      "================================================================================\n",
      "\n",
      "Most similar and dissimilar words to 'piano':\n",
      "\n",
      "Most similar                  Most dissimilar               \n",
      "------------------------------------------------------------\n",
      "deva            cosine= 0.418   postwar         cosine=-0.342\n",
      "ono             cosine= 0.414   highlighting    cosine=-0.287\n",
      "look            cosine= 0.402   auxiliary       cosine=-0.277\n",
      "respite         cosine= 0.391   about           cosine=-0.274\n",
      "rapper          cosine= 0.391   adaptable       cosine=-0.273\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "                                  Word: doctor                                  \n",
      "================================================================================\n",
      "\n",
      "Most similar and dissimilar words to 'doctor':\n",
      "\n",
      "Most similar                  Most dissimilar               \n",
      "------------------------------------------------------------\n",
      "idol            cosine= 0.436   lie             cosine=-0.400\n",
      "citing          cosine= 0.431   horizons        cosine=-0.325\n",
      "portisch        cosine= 0.409   already         cosine=-0.295\n",
      "wario           cosine= 0.405   arranged        cosine=-0.291\n",
      "corrupt         cosine= 0.385   fabricated      cosine=-0.289\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "                                 Word: musician                                 \n",
      "================================================================================\n",
      "\n",
      "Most similar and dissimilar words to 'musician':\n",
      "\n",
      "Most similar                  Most dissimilar               \n",
      "------------------------------------------------------------\n",
      "vocalist        cosine= 0.462   pennsylvanian   cosine=-0.275\n",
      "guitarist       cosine= 0.447   wildly          cosine=-0.274\n",
      "singer          cosine= 0.435   allocated       cosine=-0.268\n",
      "rapper          cosine= 0.435   catering        cosine=-0.262\n",
      "pile            cosine= 0.422   allied          cosine=-0.256\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "                                  Word: money                                   \n",
      "================================================================================\n",
      "\n",
      "Most similar and dissimilar words to 'money':\n",
      "\n",
      "Most similar                  Most dissimilar               \n",
      "------------------------------------------------------------\n",
      "institution     cosine= 0.489   pen             cosine=-0.310\n",
      "funds           cosine= 0.450   nearing         cosine=-0.289\n",
      "heine           cosine= 0.443   zoë             cosine=-0.282\n",
      "offerings       cosine= 0.432   combines        cosine=-0.267\n",
      "celebrity       cosine= 0.421   facing          cosine=-0.265\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "example_words = [\"king\", \"piano\", \"doctor\", \"musician\", \"money\"]\n",
    "\n",
    "for example in example_words:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"{'Word: ' + example:^80}\")\n",
    "    print(\"=\" * 80)\n",
    "    find_nearest(example, top_k=5)\n",
    "    print(\"\\n\" + \"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "Nous avons implémenté :\n",
    "- la **tokenisation** et la **création du vocabulaire**,\n",
    "- la **génération des couples (contexte, cible)**,\n",
    "- un **réseau de neurones CBOW** entraîné par *negative log likelihood*,\n",
    "- et l’exploration qualitative des embeddings.\n",
    "\n",
    "Ce TP illustre les fondements de l’apprentissage distributionnel du langage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
