{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab — Implementing Word2Vec (CBOW) in PyTorch\n",
    "\n",
    "**Objective:**  \n",
    "In this lab, we will build **from scratch** a Word2Vec model using the **CBOW (Continuous Bag of Words)** architecture.\n",
    "\n",
    "The goal is to learn **vector representations** (embeddings) of words by training a small neural network:\n",
    "\n",
    "$$\\mathbb{P}(w_t \\mid h_t) = \\text{Softmax}(W^{(2)} \\cdot h_t)$$\n",
    "\n",
    "where:\n",
    "\n",
    "- The average of the embeddings of the **context words** is given by  \n",
    "  $$ h_t = \\frac{1}{2C} \\sum_{i=-C, i\\neq 0}^{C} v(w_{t+i}), $$\n",
    "- $v(w) \\in \\mathbb{R}^p$ is the embedding vector of word $w$,\n",
    "- $W^{(1)} \\in \\mathbb{R}^{p \\times N}$ and $W^{(2)} \\in \\mathbb{R}^{N \\times p}$ are the model parameters,\n",
    "- And the **loss** is the negative log-likelihood:\n",
    "\n",
    "$$\\mathcal{L} = - \\sum_{t=1}^{T} \\log \\mathbb{P}(w_t \\mid h_t)$$\n",
    "\n",
    "We will train our model on a small corpus extracted from *WikiText-2*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import packages and Corpora preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = pd.read_csv(\n",
    "    './wikitext-2-train.csv',\n",
    "    header=None,\n",
    "    names=['text'],\n",
    "    encoding='utf-8'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apply a tokenization function (e.g. `tokenize_sentence`) to each non-empty text entry in `train_iter['text']`.\n",
    "- You can use `tqdm` to visualize progress.\n",
    "- Filter out sentences shorter than 3 tokens.\n",
    "- Print a small sample (e.g. first 10 tokens) from one sentence to verify the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tokenizer model \n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence):\n",
    "    return [token.text.lower() for token in nlp(sentence) if token.is_alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a vocabulary from the tokenized sentences.\n",
    "\n",
    "- Count all word occurrences across the corpus using `Counter`.\n",
    "- Keep only words that appear at least 5 times.\n",
    "- Build two dictionaries:\n",
    "  - `word2idx`: maps each word to a unique index.\n",
    "  - `idx2word`: inverse mapping from index to word.\n",
    "- Compute the total vocabulary size `V` and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "...\n",
    "V = ...\n",
    "print(f\"Taille du vocabulaire : {V}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate (context, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_target(sentences, context_size=5):\n",
    "    \"\"\"\n",
    "    Generate (context, target) training pairs for the CBOW model.\n",
    "\n",
    "    For each word in each sentence, this function extracts:\n",
    "        - A target word (the central word)\n",
    "        - Its surrounding context words within a specified window size\n",
    "\n",
    "    Example:\n",
    "        Sentence: [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\"]\n",
    "        context_size = 2\n",
    "        → Target = \"brown\"\n",
    "        → Context = [\"the\", \"quick\", \"fox\", \"jumps\"]\n",
    "\n",
    "    Args:\n",
    "        sentences (list of list of str): List of tokenized sentences\n",
    "        context_size (int): Number of context words to take on each side of the target\n",
    "\n",
    "    Returns:\n",
    "        list of tuples: Each tuple (context, target) contains\n",
    "                        - context: list of word indices (size = 2 * context_size)\n",
    "                        - target: integer index of the center word\n",
    "\n",
    "    Meaning:\n",
    "        The result is a training dataset for the CBOW neural network, where each\n",
    "        example teaches the model to predict the central word given its surrounding\n",
    "        context words.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for sentence in sentences:\n",
    "        indices = [word2idx[w] for w in sentence if w in word2idx]\n",
    "        for i in range(context_size, len(indices) - context_size):\n",
    "            context = indices[i - context_size:i] + indices[i + 1:i + context_size + 1]\n",
    "            target = indices[i]\n",
    "            data.append((context, target))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 5\n",
    "data = generate_context_target(sentences, CONTEXT_SIZE)\n",
    "print(\"Exemple :\", data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define a PyTorch Dataset for CBOW\n",
    "\n",
    "- Create a class `CBOWDataset` that inherits from `torch.utils.data.Dataset`.\n",
    "- Store all `(context, target)` pairs in the constructor (`__init__`).\n",
    "- Implement:\n",
    "  - `__len__`: returns the total number of samples.\n",
    "  - `__getitem__`: returns one sample (context and target) as tensors.\n",
    "- Keep the same method names and docstrings provided in the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for the Continuous Bag-of-Words (CBOW) model.\n",
    "    Each sample consists of a context (list of word indices) and a target word index.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with preprocessed (context, target) pairs.\n",
    "\n",
    "        Args:\n",
    "            data (list of tuples): Each tuple contains (context_indices, target_index)\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the total number of (context, target) samples.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of samples in the dataset\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve the context-target pair at the given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the desired sample\n",
    "\n",
    "        Returns:\n",
    "            tuple: (context_tensor, target_tensor)\n",
    "        \"\"\"\n",
    "        context, target = self.data[idx]\n",
    "        return torch.tensor(context, dtype=torch.long), torch.tensor(target, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CBOWDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implement the CBOW Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    \"\"\"\n",
    "    Continuous Bag-of-Words (CBOW) neural network model.\n",
    "\n",
    "    This model predicts a target word given the embeddings of its surrounding context words.\n",
    "    It consists of two linear transformations:\n",
    "        - W1: word embedding lookup table\n",
    "        - W2: projection from embedding space back to vocabulary space\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        \"\"\"\n",
    "        Initialize CBOW model parameters.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Number of unique words in the vocabulary\n",
    "            embedding_dim (int): Dimensionality of the embedding space\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ...\n",
    "\n",
    "    def forward(self, context_words):\n",
    "        \"\"\"\n",
    "        Forward pass of the CBOW model.\n",
    "\n",
    "        Args:\n",
    "            context_words (Tensor): Tensor of shape (batch_size, 2C)\n",
    "                containing indices of context words\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Log-probabilities over the vocabulary for each target word\n",
    "        \"\"\"\n",
    "        ...\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW(\n",
      "  (W1): Embedding(19402, 100)\n",
      "  (W2): Linear(in_features=100, out_features=19402, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "model = CBOW(V, embedding_dim)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Loop for the CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS):\n",
    "    ...\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Explore the Learned Embedding Space\n",
    "\n",
    "**Goal:** Write a function to inspect which words are most and least similar in the learned embedding space.\n",
    "\n",
    "**Instructions:**\n",
    "- Define a function `find_nearest(word, top_k=5)` that:\n",
    "  - Checks if the word exists in the vocabulary; if not, print an informative message.\n",
    "  - Retrieves the embedding vector of the given word from the model.\n",
    "  - Computes cosine similarities between this vector and all other word embeddings in `model.W1.weight`.\n",
    "  - Identifies:\n",
    "    - The **top-k most similar** words (highest cosine similarity).\n",
    "    - The **top-k most dissimilar** words (lowest cosine similarity).\n",
    "  - Prints both lists side by side for comparison.\n",
    "- Keep the **method name** and **docstring** exactly as in the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(word, top_k=5):\n",
    "    \"\"\"\n",
    "    Display the top-k most similar and least similar words to a given word\n",
    "    based on cosine similarity in the embedding space.\n",
    "\n",
    "    Args:\n",
    "        word (str): The query word.\n",
    "        top_k (int): Number of top and bottom words to display.\n",
    "    \"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_words = [\"king\", \"piano\", \"doctor\", \"musician\", \"money\"]\n",
    "\n",
    "for example in example_words:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"{'Word: ' + example:^80}\")\n",
    "    print(\"=\" * 80)\n",
    "    find_nearest(example, top_k=5)\n",
    "    print(\"\\n\" + \"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "We have implemented:\n",
    "- **Tokenization** and **vocabulary construction**,  \n",
    "- **Generation of (context, target)** pairs,  \n",
    "- A **CBOW neural network** trained using *negative log-likelihood*,  \n",
    "- And a **qualitative exploration** of the learned embeddings.\n",
    "\n",
    "This practical session illustrates the fundamental principles of **distributional language learning**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
